{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a neural network gets some information then not all the information is useful to be passed on. So `activation function helps the network to pass on the relevant information and restricts the irrelevant information`.\n",
    "\n",
    "\n",
    "* The activation function is a mathematical `gate` in between the input feeding the current neuron and its output going to the next layer. \n",
    "\n",
    "\n",
    "* `Activation functions helps to determine the output of a neural network`. These type of functions are attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "\n",
    "> `Activation function also helps to normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.`\n",
    "\n",
    "\n",
    "<img src=\"Images_for_Activation&Loss/1.jpg\">\n",
    "\n",
    "\n",
    "* In a neural network, inputs are fed into the neurons in the input layer. Each neuron has a weight, and multiplying the input number with the weight gives the output of the neuron, which is transferred to the next layer.\n",
    "\n",
    "\n",
    "* In a neural network each neuron is characterized by its `weight`, `bias` and `activation function`.\n",
    "\n",
    "\n",
    "* When a neuron gets an input it perform a linear transformation on this input using the weights and biases.\n",
    "\n",
    "\n",
    "$$linearTransformation = (Weights * Input) + Bias$$\n",
    "\n",
    "\n",
    "* Now the above linearTransformation is passed into the activation function which gives out the output of neuron.\n",
    "\n",
    "\n",
    "$$Output = Activation( \\sum linearTransformation)$$ \n",
    "\n",
    "\n",
    "\n",
    "* As without an activation function, a neural network will be like linear regression model. Though this would make our neural network simple but our network won't be able to learn complex pattern from our data. `Thus we use a non linear transformation to the inputs of the neuron and this non-linearity in the network is introduced by an activation function.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need Activation Functions?\n",
    " \n",
    "* Without activation function, weight and bias would only have a linear transformation, or neural network is just a linear regression model, a linear equation is polynomial of one degree only which is simple to solve but limited in terms of ability to solve complex problems or higher degree polynomials.  \n",
    "\n",
    " \n",
    "\n",
    "* But opposite to that, the addition of activation function to neural network executes the non-linear transformation to input and make it capable to solve complex problems such as language translations and image classifications. \n",
    "\n",
    " \n",
    "\n",
    "* In addition to that, `Activation functions are differentiable due to which they can easily implement back propagations`, optimized strategy while performing backpropagations to measure gradient loss functions in the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic process carried out by a neuron in a neural network\n",
    "\n",
    "![](https://missinglink.ai/wp-content/uploads/2018/11/Basic-process-carried-out-by-a-neuron-in-a-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a `threshold based classifier` i.e. the neuron is activated based on the value from the linear transformation.\n",
    "\n",
    "\n",
    "* Here `if the value of the linear transformation is greater than a threshold, then the neuron is activated, else it is deactivated`, i.e. its output is not considered for the next hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\text{f(x):}\n",
    "\\begin{cases}\n",
    "1&\\text{for $x\\ge threshold$}\\\\\n",
    "0&\\text{for $x\\lt threshold$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAP0AAADHCAMAAADlCqUFAAAAvVBMVEX///+MjIzz8/9dX/+0tv+Fif+Pj4/b29sAAP/w8PBtbW2cnf8AAAD8/PxNTU329vbKysp5eXni4uLY2Ng3NzciIiJTU1P4+P/Dw8Pq6up/f3+zs7NgYGCGhoZERERoaGj19e+gov8XFxeXl5efn59vb2iPj4c8PtthY/8zNeesr/+5u/9jZbAvLy8AAPNubXW3t7dlZFtVV8eEhuk3OKyUlv+kpcpJTMaOkf8+QbJ/gvaDhLMQEBAeHh6qqqqRtl4NAAADhElEQVR4nO3dW3OTUBSG4UXVTUHYm2MCBIgmsR6reKjn5v//LEkcR6/3cvK1k++9aK8We55ASDtDQKwJFOWVajyoB820GVWLGytGNIVr1biMTjXe6VY3EqjmC92Ll9pINb9LVeOBUi+65dNYp9etrtfr0uqVUQ+MemDUA6MeGPXAqAdGPTDqgVEPjHpg1AOjHhj1wKgHRj0w6oFRD4x6YNQDox4Y9cCoB0Y9MOqBUQ+MemDUAztjfXTO+ih3eL3uImv/wsS6NH32BLT8sSuxW6O7Rt6ztFg4WT5/AOzFS8mLUHmdt2+LSC4uXz3E9foN7MgXsQf9Era8HI583Dl/cLP+Ara8wD/vqQdGPTDqgVEPjHpg1AOjHhj1wKgHRj0w6oFRD4x6YNQDox4Y9cCoB0Y9MOqBUQ+MemDUA6MeGPXAqAdGPTDqgVEP7Az1LiuOP7PsDPVpfn0987Okru6C/sSX57eJ7GORXdwcr9EGfTlADrefD6SK49N+NyOz0sz6wPbzvn/7rm9PuvrfXBxXp9/3xbWsZn0r6Vgu0fv+9O97G1TTvun6ob4L7/tT164myUo3TeE56v+JemDUA6MeGPXAqAdGPTDqgVEPjHpg1AOjHhj1wKgHRj0w6oFRD4x6YNQDox4Y9cCoB0Y9MOqBUQ+MemDUA6MeGPXA7rk+alTjWn2mu8I7kCvVfPheNb5U3j8/1+mNfHj6WNHHT6rxm8ubz2XoXbFWDJf7VS9fHqn6qhv/9v3HlQHVDaPyyNeGfWaKwZ7z4c/L8dY79/un8zzzHOY1+vS4sP/6otGXm9vDaJ/8LL3mi7o2Gn1bbfL5l02Sve8mFPp82yZO2jHzfNxOvi3qSPz1pnN2ft3rSfG4H0+9C6N+5TahlMlt7bfvbZMuGoV+3chi3uvJZpP5bsJXH/bbxSpNWokK2XdeK8/6ed/56+OV9JO4UqbRdxOKIz+IV0nUZIui33rNb+MpEYV+P2ZVOxWLolv7bkKhT4eujAbZ9cYP4IKu0Zz13C6fnHG7fvD/2ND/j6f6S/s/fN5r1ueTIYFRD4x6YNQDox4Y9cCoB0Y9MOqBUQ+MemDUA6MeGPXAqAdGPTDqgVEPjHpg1AOjHhj1wKgHds/1qe7e+1q98s7/Wn3od7Hen1Kr0xvtdzOMaj70v1ju2Kjbe7rXfrZbEyjKK9V4UA+aaTOqFjf2F+zzRHvSKr1kAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b> Here in the above image we have `threshold= 0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used only for `Binary class classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. As the name suggests `it can only be used for the classification of data having two classes in output` as the binary step function gives only two output and hence, can be used as an activation function while creating a binary classifier. It `can't be used when there are multiple classes in the target variable`. \n",
    "\n",
    "\n",
    "* 2. As gradients are calculated to update the weights and biases during the backpropagation process. Since the `gradient of the step function is zero` which causes a hinderance in the back propagation process. (i.e. if you calculate the derivative of f(x) with respect to x, it comes out to be 0), thus, `the weights and biases don’t get update`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we saw that the gradient or derivation of the Binary step function gives 0, as there is no component of x in the binary step function. And due to this the weights won't get update during the backpropagation.\n",
    "\n",
    "\n",
    "* So to overcome this, we came up with the `Linear function` which is as follows:\n",
    "\n",
    "$$f(x) = mx$$\n",
    "\n",
    "\n",
    "![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOsAAACfCAMAAAAvZ94wAAAACVBMVEX///8AAAD/AADAyZ2wAAACvElEQVR4nO3dAW+CMBCGYc7//6MXtoKFgbbl2rvvu76LW9QZfBROIwkuyyxwItuvAP1hre/FoCQONZg1DDXMYFqbVL7mtjqbzZwWaYsNZJVAoziQVSQONpBVJA42kFUkDjaSdS2Kc21aOZtWzqys9zuS1gvfJ1H8oMjsec12JJ1e+mT/WXQ/ErNbh+92rvwiZbtW8/45tOZUGuu+0h7WYclOup9gG1qvF/2mivZORNN1+O7ibUZvp75L7J/Bgm2sNjuSjKwmix290Nfg5eUNtlpSB1tNqUOtL1vqSKuxdKTVnDrOak8dZnVAHWX1QB1jtR7AqRFWH9IhVi/UAVY31P5WP9TuVkfUzlYnAzjV1epK2tfqjNrT6o3a0eqO2s/qj9rL6msAp/pYPUo7WX1Su1idUntYvVI7WN1S1a0uB3BK2epYqm11TdW1+qaqWp1TNa3eqXpWzwM4pWX1L1WzIlCVrBBUHSsGVcUKQlWwAgzg1GMrjPS5FYj61IpEfWiFoj6zYlGfWHEGcKrdiiZ9YMWjNlsBqa1WRGqjFZLaZsWktljhXmu26q2o0gYrLrXaCkyttSJTK63Q1Cor7ABOVVjBpTVWeGq5FZ9abCWglloZqGVW9AGcKrFySIusLNQCKw31u5WH+tVKRP1iJRnAqY9WKulnKxn1k5WN+sFKR7238lHvrFwDOHVtZZTeWDmpl1ZS6pWVlXphpaX+s1IO4NTJSiw9W6mpRys39WAlp+ZWdurbyjyAU5uVX7pbI1CTNQT1z4pCfXjI1/XWKNTl+D0p1QfEF6QBfP5WmMrnWXCk54OoZy+YUtSr7N+8dbIWPlQ6j/iYjtLqI3EDWvdzpfd+uxGcNTtTOJx2J5i15Ub7HyhrS++Ne1qJqp9kuGVP67QSlb/DYrfmSnrr0vz2Gbpp5WxaOZvW2Ww2sh9ZKwMxnOPlKAAAAABJRU5ErkJggg==)\n",
    "\n",
    "\n",
    "<center><b>The variable ‘m’ is the slope of the line and can be any constant value.</b></center>\n",
    "\n",
    "\n",
    "* Here the `activation function` f(x) is `proportional to the input 'x'`.\n",
    "\n",
    "\n",
    "* Linear function are `ideal for simple tasks where interpretability is highly desired`.\n",
    "\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "* Although the `gradient` or derivation wrt 'x' here does not become zero, but it is a constant value 'm' that `doesn't depend upon the input 'x'` at all. This concludes that the weights and biases will be updated during the backpropagation process but the updating factor would be the same i.e. 'm' always.\n",
    "\n",
    "\n",
    "* In this scenario, the gradient is the same for every iteration and thus, the neural network will not really improve the error. The network will not be able to train well and capture the complex patterns from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is one of the most widely used `non-linear activation function`. This basically concludes that when we have multiple neurons having sigmoid function as their activation function then, the `output is non linear` as well.\n",
    "\n",
    "\n",
    "* The sigmoid function is as follows:\n",
    "\n",
    "![alt](Images_for_Activation&Loss/sig.svg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "<center><b> The range of the sigmoid function  is asymptotic [0,1] </center></b>\n",
    "\n",
    "\n",
    "* It is a smooth S-shaped function and is `continuously differentiable`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Sigmoid function is the most frequently used activation function in the beginning of deep learning. It is a smoothing function that is easily deriviable.\n",
    "\n",
    "\n",
    "* In the sigmoid function, we can see that `its output is in the open interval (0,1)`. \n",
    "\n",
    "\n",
    "* `We can think it of as a probability, but in the strict sense, don't treat it as probability`. The sigmoid function was once more popular. It can be thought of as the firing rate of a neuron. In the middle where the slope is relatively large, it is the sensitive area of the neuron. On the sides where the slope is very gentle, it is the neuron's inhibitory area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After derivation or gradient of the sigmoid function\n",
    "\n",
    "![](https://www.researchgate.net/profile/Yalin_Bastanlar/publication/309379147/figure/fig2/AS:430806826000388@1479723759988/Derivative-of-Sigmoid-Function.png)\n",
    "\n",
    "<center><b> The range of the derivative of sigmoid function  is asymptotic (0, 0.25]</center></b>\n",
    "    \n",
    "    \n",
    "* The `gradient values are significant for range -3 and 3` but the graph gets much flatter in other regions. This implies that `for values greater than 3 or less than -3, will have very small gradients`. As the gradient value approaches zero, the network is not really learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages: \n",
    "\n",
    "1. Smooth gradient, preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations:\n",
    "\n",
    "*  The `sigmoid` function is `not symmetric around zero`. So `output of all the neurons will be of the same sign`. This can be addressed by scaling the sigmoid function which is exactly what happens in the tanh function.\n",
    "\n",
    "\n",
    "* **Vanishing gradient** — For very high or very low values of X, there is almost no change to the prediction, causing a `vanishing gradient problem`. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction. `When the input is slightly away from the coordinate origin`, the `gradient` of the function `becomes` very small, `almost zero`. In the process of neural network backpropagation, we all use the chain rule of differential to calculate the differential of each weight w. When the backpropagation passes through the sigmod function, the differential on this chain is very small. Moreover, it may pass through many sigmod functions, which will eventually cause the weight w to have little effect on the loss function, which is `not conducive to the optimization of the weight`. This The problem is called gradient saturation or gradient dispersion or `Vanishing Gradient`.\n",
    "\n",
    "\n",
    "\n",
    "* The function output is not centered on 0, which will reduce the efficiency of weight update.\n",
    "\n",
    "\n",
    "* Computationally expensive. The sigmod function performs exponential operations, which is slower for computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tanh ( Hyperbolic Tangent Activation Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a S-shaped `non-linear activation function` similar to sigmoid activation function, with only difference that `it is symmetric about origin`, so the `output of all the neurons will not be of the same sign`.\n",
    "\n",
    "\n",
    "* The tanh function is as follows:\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSed8QBoO_Ael741OAxJuATdVq-OLJ_cUWyC4dEunpnH6Moi6hy&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRSkauKG6l-gXoycnpGyIfJZBYAWCZ4t0V0FavFmU7UkDCi4inl&usqp=CAU)\n",
    "\n",
    "\n",
    "<center><b> The range of the tanh function  is asymptotic [-1,1] </center></b>\n",
    "\n",
    "\n",
    "* It is a smooth S-shaped function and is `continuously differentiable`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of tanh\n",
    "\n",
    "![](https://keisan.casio.com/keisan/lib/real/system/2006/15411343272927/detanh.png)\n",
    "\n",
    "\n",
    "<center><b> The range of the derivative of tanh function  is asymptotic (0, 1]</center></b>\n",
    "\n",
    "\n",
    "* The gradient of the tanh function is steeper as compared to the sigmoid function.\n",
    "\n",
    "\n",
    "* `Usually tanh is preferred over the sigmoid function since it is zero centered and the gradients are not restricted to move in a certain direction.`\n",
    "\n",
    "\n",
    "Tanh is a hyperbolic tangent function. The curves of tanh function and sigmod function are relatively similar. Let ’s compare them. First of all, `when the input is large or small, the output is almost smooth and the gradient is small, which is not conducive to weight update`. The difference is the output interval. \n",
    "\n",
    "\n",
    "* The `output interval` of tanh is `(-1, +1)`, and the whole function is 0-centric, which is better than sigmod.\n",
    "\n",
    "\n",
    "**`In general binary classification problems, the tanh function is used for the hidden layer and the sigmod function is used for the output layer. However, these are not static, and the specific activation function to be used must be analyzed according to the specific problem, or it depends on debugging.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a  non-linear activation function.\n",
    "\n",
    "\n",
    "* The main advantage of using the ReLU function over other activation functions is that `it does not activate all the neurons at the same time`. This is because of the nature of the graph it has. \n",
    "\n",
    "\n",
    "* `ReLU function should only be used in the hidden layers.`\n",
    "\n",
    "\n",
    "* The Relu function is as follows:\n",
    "\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRd_qrzNxqTaqNUAROI8-AklNsH3QFK_8YuUb5ASWuGrXkAiGJ8&usqp=CAU)\n",
    "\n",
    "\n",
    "* The function can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f(x):}\n",
    "\\begin{cases}\n",
    "x&\\text{for $x\\ge 0$}\\\\\n",
    "0&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "* `This shows that the neurons will only be deactivated if the output of the linear transformation is less than 0 and activated only if it's greater than 0`.\n",
    "\n",
    "\n",
    "* For the negative input values, the result is zero, that means the neuron does not get activated. Since only a certain number of neurons are activated, <b>`the ReLU function is far more computationally efficient when compared to the sigmoid and tanh`</b> function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of ReLU\n",
    "\n",
    "* The function of gradient of ReLU can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f '(x):}\n",
    "\\begin{cases}\n",
    "1&\\text{for $x\\ge 0$}\\\\\n",
    "0&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1424/1*VlNcOq94nrPxpwd9JdQ1QQ.png)\n",
    "\n",
    "\n",
    "\n",
    "### Advantages:\n",
    "1. When the input is positive, there is `no gradient saturation/ vanishing gradient problem`.\n",
    "\n",
    "\n",
    "2. The calculation speed is much faster. The ReLU function has only a linear relationship. Whether it is forward or backward, it is`much faster than sigmod and tanh`. (Sigmod and tanh need to calculate the exponent, which will be slower.)\n",
    "\n",
    "### Limitations:\n",
    "1. We can see that for <b>x<0</b> the gradient is 0, due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated. `This is taken care of by the ‘Leaky’ ReLU function.`\n",
    "    \n",
    "    \n",
    "2. When the `input is negative, ReLU is completely inactive`, which means that once a negative number is entered, ReLU will die. In this way, in the forward propagation process, it is not a problem. Some areas are sensitive and some are insensitive. But in the backpropagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the sigmod function and tanh function.\n",
    "\n",
    "\n",
    "3. We find that the output of the ReLU function is either 0 or a positive number, which means that the `ReLU function is not a 0-centric function.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is an improved version of ReLU.\n",
    "\n",
    "\n",
    "* As we saw that for the ReLU function, the gradient is 0 for x<0, which would deactivate the neurons in that region. So to resolve this issue, `instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x`.\n",
    "\n",
    "\n",
    "* The leaky Relu functions is as follows:\n",
    "\n",
    "$$f(x) = max(0.01*x , x)$$\n",
    "\n",
    "\n",
    "\n",
    "* The function can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f(x):}\n",
    "\\begin{cases}\n",
    "x&\\text{for $x\\ge 0$}\\\\\n",
    "0.01*x&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://i0.wp.com/knowhowspot.com/wp-content/uploads/2019/04/IMG_20190406_220045-1.jpg?fit=1024%2C561&ssl=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient of Leaky ReLU\n",
    "\n",
    "\n",
    "* `By making this small modification, the gradient of the left side of the graph comes out to be a non zero value. Hence we would no longer encounter dead neurons in that region.`\n",
    "\n",
    "\n",
    "* The function of gradient of Leaky ReLU can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f '(x):}\n",
    "\\begin{cases}\n",
    "1&\\text{for $x\\ge 0$}\\\\\n",
    "0.01&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PReLU (Parameterised ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also an improved version of ReLU to resolve the issue of 0 gradient on the values of x<0.\n",
    "\n",
    "\n",
    "* In this a new parameter `a` is a slope of the negative part of the function is introduced. \n",
    "\n",
    "\n",
    "* The PReLU functions is as follows:\n",
    "\n",
    "$$f(x) = max(a*x , x)$$\n",
    "\n",
    "\n",
    "<center><b>'a' is the new parameter as slope</center></b>\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS9Yxoi5W34-ltyMjALGC7TSOMoBW4DM4-6I1Yne1Hm4I4DyfPC&usqp=CAU)\n",
    "\n",
    "\n",
    "* The function can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f(x):}\n",
    "\\begin{cases}\n",
    "x&\\text{for $x\\ge 0$}\\\\\n",
    "a*x&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "* `When the value of a is fixed to 0.01, the function acts as a Leaky ReLU function. However, in case of a parameterised ReLU function, ‘a‘ is also a trainable parameter. The network also learns the value of ‘a‘ for faster and more optimum convergence.`\n",
    "\n",
    "\n",
    "* <b>`The parameterized ReLU function is used when the leaky ReLU function still fails to solve the problem of dead neurons and the relevant information is not successfully passed to the next layer.`</b>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient of Parameterised ReLU\n",
    "\n",
    "* The function of gradient of Parameterised ReLU can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f '(x):}\n",
    "\\begin{cases}\n",
    "1&\\text{for $x\\ge 0$}\\\\\n",
    "a&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ELU (Exponential Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also an improved version of ReLU to resolve the issue of zero gradient on the values of x<0.\n",
    "\n",
    "\n",
    "* Unlike the Leaky relu and Parametric ReLU functions, instead of a straight line, ELU uses a log curve for defining the negative values.\n",
    "\n",
    "\n",
    "* The function for ELU can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f(x):}\n",
    "\\begin{cases}\n",
    "x&\\text{for $x\\ge 0$}\\\\\n",
    "a(e^x -1)&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://i.stack.imgur.com/7aiVh.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of ELU\n",
    "\n",
    "* The function of gradient of ELU can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{f '(x):}\n",
    "\\begin{cases}\n",
    "1&\\text{for $x\\ge 0$}\\\\\n",
    "ae^x&\\text{for $x\\lt 0$}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1476/1*L7VDcOPLMneyVmuzQ4GCpw.png)\n",
    "\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. No Dead ReLU issues\n",
    "\n",
    "\n",
    "2. The mean of the output is close to 0, zero-centered\n",
    "\n",
    "\n",
    "One small problem is that it is slightly more computationally intensive. Similar to Leaky ReLU, although theoretically better than ReLU, `there is currently no good evidence in practice that ELU is always better than ReLU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Swish (A self-gated function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Swish is as computationally efficient as ReLU and `shows better performance than ReLU on deeper models`.  \n",
    "\n",
    "\n",
    "* The values for swish `ranges from negative infinity to infinity`. \n",
    "\n",
    "\n",
    "* The swish function is as follows:\n",
    "\n",
    "\n",
    "$$f(x) = x * sigmoid(x)$$\n",
    "\n",
    "\n",
    "* It can be rewritten as:\n",
    "\n",
    "\n",
    "$$f(x) = \\frac{x}{1 + e ^-x}$$\n",
    "\n",
    "\n",
    "![](https://missinglink.ai/wp-content/uploads/2018/11/swish.png)\n",
    "\n",
    "\n",
    "* Swish function is smooth and `is differentiable at all points`, due to which it is helpful during the model optimization process and is considered to be one of the reasons that swish outperforms ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Swish's design was inspired by the `use` of sigmoid functions `for gating in LSTMs and highway networks`. We use the same value for gating to simplify the gating mechanism, which is called **self-gating**. \n",
    "\n",
    "\n",
    "* The advantage of self-gating is that it only requires a simple scalar input, while normal gating requires multiple scalar inputs. This feature enables self-gated activation functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "\n",
    "1) Unboundedness (unboundedness) is helpful to prevent gradient from gradually approaching 0 during slow training, causing saturation. At the same time, being bounded has advantages, because bounded active functions can have strong reguairzation, and larger negative inputs will be resolved.\n",
    "\n",
    "\n",
    "2) At the same time, smoothness also plays an important role in optimization and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Softmax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is used for `multiclass classification problems.`\n",
    "\n",
    "\n",
    "* Softmax activation function `returns the probability for a datapoint belonging to each individual class`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/3042/1*G409bxCMO42RYNI4s7QaQw.png)\n",
    "\n",
    "\n",
    "\n",
    "![](https://pbs.twimg.com/media/DYwPcIXX4AAoYwx.jpg)\n",
    "\n",
    "\n",
    "* While building a network for a multiclass problem, `the output layer would have as many neurons as the number of classes in the target.`\n",
    "\n",
    "\n",
    "* `For instance if you have three classes, there would be three neurons in the output layer.`\n",
    "\n",
    "\n",
    "* Suppose you got the output from the neurons as [1.2 , 0.9 , 0.75]. Applying the softmax function over these values, you will get the following result – [0.42 ,  0.31, 0.27]. `These represent the probability for the data point belonging to each class.` Note that the sum of all the values is 1.\n",
    "\n",
    "\n",
    "![](https://www.machinecurve.com/wp-content/uploads/2020/01/softmax_logits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for an arbitrary real vector of length K, Softmax can compress it into a real vector of length K with a value in the range (0, 1), and the sum of the elements in the vector is 1.\n",
    "\n",
    "\n",
    "* It also has many applications in Multiclass Classification and neural networks Softmax is different from the normal max function: the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly. It is a \"max\" that is \"soft\".\n",
    "\n",
    "\n",
    "* Denominator of the Softmax function combines all factors of the original output value, which means that the different probabilities obtained by the Softmax function are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](Images_for_Activation&Loss/soft31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Maxout\n",
    "\n",
    "The Maxout activation function is defined as follows\n",
    "\n",
    "![alt](Images_for_Activation&Loss/max.jpeg)\n",
    "\n",
    "* One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1 =0).The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks\n",
    "\n",
    "\n",
    "* `The Maxout activation is a generalization of the ReLU and the leaky ReLU functions. `\n",
    "\n",
    "\n",
    "* `It is a learnable activation function.`\n",
    "\n",
    "\n",
    "* Maxout can be seen as adding a layer of activation function to the deep learning network, which contains a parameter k. Compared with ReLU, sigmoid, etc., this layer is special in that it adds k neurons and then outputs the largest activation value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm.\n",
    "\n",
    "\n",
    "* `As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly`. This requires the choice of an error function, also known as `loss function, that can be used to estimate the loss of the model so that the weights can be updated to reduce the loss on the next evaluation`.\n",
    "\n",
    "\n",
    "* Suppose you’ve trained a machine learning model on a given dataset and you will present it in front of your client. But how can you make sure that the model you've built will give the optimum result? This is where loss functions come into play in machine learning.\n",
    "\n",
    "\n",
    "* `Loss Function` is a metric or a technique that will help you quickly evaluate your model on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you went on a trek and reached the top of a hill and now you need need to climb down. How do you decide where to walk towards?\n",
    "\n",
    "\n",
    "*  Look around to see all the possible paths\n",
    "\n",
    "\n",
    "* Reject the ones going up. This is because these paths would actually cost me more energy and make my task even more difficult\n",
    "\n",
    "\n",
    "* Finally, take the path that I think has the most slope downhill as going down is beneficial for us Therefore, it has a negative cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What’s the Difference between a Loss Function and a Cost Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Loss function` is for a single training example. It is also sometimes called an `Error function`. \n",
    "\n",
    "\n",
    "A `Cost function`, on the other hand, is the average loss over the entire training dataset. \n",
    "\n",
    "\n",
    "**`NOTE: The optimization techniques basically aim at minimizing the Cost function as our model is trained over entire training set not on a single dataset.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of Loss Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of loss function, each of which are used based on the type of model we have:->\n",
    "\n",
    "> * Regression Loss Functions\n",
    "\n",
    "> * Binary Classification Loss Functions\n",
    "\n",
    "> * Multi-class Classification Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regression Loss Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This we basically use in `Linear Regression` where we model a linear relationship between a dependent variable, Y, and several independent variables, X_i’s. Thus, we essentially fit a line in space on these variables.\n",
    "\n",
    "\n",
    "$$Y = a_0 + a_1 * X_1 + a_2 * X_2 + ...........+ a_n * X_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With the dataset is provided to us, we need to find the value of: $a_0$ , $a_1$, .... $a_n$ such that the line passing through the given data points should have minimum distance from all the data points.\n",
    "\n",
    "\n",
    "![](https://katbailey.github.io/ml/images/linear-regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Mean Absolute Error Loss:\n",
    "\n",
    "* It is also known as `L1 Loss` or `Least Absolute Deviations`.\n",
    "\n",
    "\n",
    "* It is used in regression problems where the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
    "\n",
    "\n",
    "* It is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
    "\n",
    "\n",
    "* `Absolute Error loss` for each training example is the `distance between the predicted and the actual values`, irrespective of the sign.\n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/mae.jpg.jpg)\n",
    "\n",
    "\n",
    "* Here the `cost function` is the `Mean of Absolute errors i.e. (MAE)`\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS-1lyQK74Qh-qxeBOtgR7nwjvvSlkoBw1psHs3ac3xHJl0otH4&usqp=CAU)\n",
    "\n",
    "\n",
    "* The MAE cost function is `more robust to outliers as compared to MSE`. However, `handling the absolute or modulus operator in mathematical equations is not easy`. We can consider this as a disadvantage of MAE.\n",
    "\n",
    "\n",
    "* The mean absolute error loss function can be used `in Keras by specifying ‘mae‘ or 'mean_absolute_error' as the loss function` when compiling the model.\n",
    "\n",
    "\n",
    "<center><b>  model.compile(loss='mean_absolute_error') </b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Mean squared Error Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is the default loss to use for regression problems.\n",
    "\n",
    "\n",
    "* It is also known as `L2 Loss`.\n",
    "\n",
    "\n",
    "* `Squared Error loss` for each training example, is the `square of the difference between the actual and the predicted values`.\n",
    "\n",
    "\n",
    "* `The result is always positive` regardless of the sign of the predicted and actual values and a `perfect value is 0.0`. \n",
    "\n",
    "\n",
    "* **`The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.`**\n",
    "\n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/mse.jpg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The corresponding cost function is the `Mean of these Squared Errors` or `Mean Squared Error (MSE)`\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQiPOMfDdL0rxVA1QcrP4KkDuiq7TS7fJ_6TpgksIMCHRvF0klJ&usqp=CAU)\n",
    "\n",
    "\n",
    "*  It is the loss function to be evaluated first and only changed if you have a good reason.\n",
    "\n",
    "\n",
    "* The `MSE cost function penalizes the model for making large errors by squaring them`. Squaring a large quantity makes it even larger. But there’s a disadvantage that this property makes the `MSE cost function less robust to outliers`. `Therefore, it should not be used if our data is prone to many outliers.`\n",
    "\n",
    "\n",
    "* The mean squared error loss function can be used `in Keras by specifying ‘mse‘ or ‘mean_squared_error‘ as the loss function` when compiling the model.\n",
    "\n",
    "\n",
    "<center><b>  model.compile( loss= 'mean_squared_error' ) </center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Mean Squared Logarithmic Error Loss:\n",
    "\n",
    "* When we see the formulation of the MSE, it just looks like a difference of a log function. In reality, that `small difference of log is the primary factor that gives MSLE the unique properties of its own`.\n",
    "\n",
    "\n",
    "#### Robustness to the effect of the outliers:\n",
    "\n",
    "> In the case of MSE, the presence of outliers can explode the error term to a very high value. But, `in the case of MSLE the outliers are drastically scaled down therefore nullifying their effect`.\n",
    "\n",
    "\n",
    "> Let’s understand this with a small example:\n",
    "\n",
    "\n",
    ">  Consider the predicted value to be $Y_P$ and the actual value to be $Y_A$\n",
    "$$Y_A = 60\\ 80\\ 90$$\n",
    "$$Y_P = 67\\ 78\\ 91$$\n",
    "\n",
    "\n",
    "> On calculating, the value of RMSE= 4.242 and RMSLE= 0.6466 \n",
    "\n",
    "\n",
    ">  Now let us introduce an outlier in the data:\n",
    "\n",
    "\n",
    "$$Y_A = 60\\ 80\\ 90\\ 750$$\n",
    "$$Y_P = 67\\ 78\\ 91\\ 102$$\n",
    "\n",
    "\n",
    "> Now, in this case, the RMSE= 374.724 and RMSLE= 1.160\n",
    "\n",
    "\n",
    "\n",
    "> We can clearly see that the `value of the MSE explodes in magnitude as soon as it encounters an outlier`. In contrast, even on the `introduction of the outlier, the MSLE error is not affected much`. From this small example, we can clearly infer that RMSLE is very robust when outliers come into play.\n",
    "\n",
    "\n",
    "#### Relative Error:\n",
    "\n",
    "* It is fundamentally a calculation relative error.\n",
    "\n",
    "\n",
    "<img src=\"Images_for_Activation&Loss/01.png\">\n",
    "\n",
    "\n",
    "* We can clearly see that due to the property of Logarithms, the MLSE can be broadly seen as relative Error between the predicted and the actual values.\n",
    "\n",
    "\n",
    "* Let’s understand this with the help of an example.\n",
    "\n",
    "\n",
    "> Case 1:\n",
    "\n",
    "$$Y_A = 100$$\n",
    "$$Y_P = 90$$\n",
    "\n",
    "\n",
    "Calculated RMLSE: 0.1053\n",
    "Calculated RMSE: 10\n",
    "\n",
    "\n",
    "> At first glance, nothing looks flashy. Let’s consider another example.\n",
    "\n",
    "\n",
    "> Case 2:\n",
    "\n",
    "$$Y_A = 10000$$\n",
    "$$Y_P = 9000$$\n",
    "\n",
    "\n",
    "Calculated RMSLE: 0.1053\n",
    "Calculated RMSE : 1000\n",
    "\n",
    "\n",
    "Surprised?\n",
    "\n",
    "\n",
    "These two examples perfectly support the argument of the relative error which we mentioned above, `RMSLE metric only considers the relative error between the Predicted and the actual value and the scale of the error is not significant. On the other hand, RMSE value Increases in magnitude if the scale of error increases.`\n",
    "\n",
    "\n",
    "### Biased Penalty:\n",
    "\n",
    "* This is the most important property for the introduction of MSLE.\n",
    "\n",
    "\n",
    "* `RMSLE incurs a larger penalty for the underestimation of the Actual variable than the Overestimation`.In simple words, `more penalty` is incurred when the `predicted Value is less than the Actual Value`. On the other hand, `Less penalty` is incurred when the `predicted value is more than the actual value`.\n",
    "\n",
    "\n",
    "<img src=\"Images_for_Activation&Loss/02.png\">\n",
    "\n",
    "\n",
    "* From these two cases, it is evident that the RMLSE incurs a larger penalty for the underestimation of the actual value. `This is especially useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated`.\n",
    "\n",
    "\n",
    "* **For example:--> Consider a Regression problem where we have to predict the time taken by an agent to deliver the food to customers.Now, if the Regression model which we built overestimates the delivery time, the delivery agent then gets a relaxation on the time he takes to deliver food and this small overestimation is acceptable.But the problem arises when the predicted delivery time is less than the actual trip takes, in this case, the delivery agent is more likely to miss the deadline, as a result, the customer reviews can be affected.**\n",
    "\n",
    "\n",
    "<img src=\"Images_for_Activation&Loss/03.png\">\n",
    "\n",
    "\n",
    "* The mean squared logarithmic error loss function can be used `in Keras by specifying ‘msle‘ or 'mean_squared_logarithmic_error' as the loss function` when compiling the model.\n",
    "\n",
    "\n",
    "<center><b>  model.compile( loss= 'mean_squared_logarithmic_error' ) </center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Huber Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Huber loss combines the best properties of MSE and MAE. \n",
    "\n",
    "\n",
    "* `It is quadratic for smaller errors and is linear otherwise`. The same is followed by its gradient or derivative. \n",
    "\n",
    "\n",
    "* Huber Loss is often used in regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers(because if the residual is too large, it is a piecewise function, loss is a linear function of the residual).\n",
    "\n",
    "\n",
    "* It is identified by its delta parameter.\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQCejpgzvd8OuWecvYyF0JMslxRAJcvVso8355XnuxbBbIxBd28&usqp=CAU)\n",
    "\n",
    "\n",
    "* `Huber loss is more robust to outliers than MSE`. It is used in Robust Regression, M-estimation and Additive Modelling.\n",
    "\n",
    "\n",
    "### Advantage:\n",
    "\n",
    "\n",
    "* `When the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary Classification Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binary Classification refers to assigning an object into one of two classes. In other words we can say it as those predictive modeling problems where examples are assigned one of two labels.\n",
    "\n",
    "\n",
    "* The problem is often framed as predicting a value of 0 or 1 for the first or second class and is often implemented as predicting the probability of the example belonging to class value 1.\n",
    "\n",
    "\n",
    "* `This classification is based on a rule applied to the input feature vector`. \n",
    "\n",
    "For example: Classifying a tweet into positive or negative sentiments or classifying an email as spam or not spam based on, say its subject line or classifying a tumor as ‘Malignant’ or ‘Benign’ based on features like average radius, area, perimeter, etc is binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross-entropy is the `default loss function to be used in case of binary classification problems`.\n",
    "\n",
    "\n",
    "* It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
    "\n",
    "\n",
    "* `Entropy` to indicate disorder or randomness or uncertainty. It is measured for a random variable X with probability distribution p(X):\n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/entropy.jpg1.jpg)\n",
    "\n",
    "\n",
    "* The negative sign is used to make the overall quantity positive.\n",
    "\n",
    "\n",
    "* `A greater value of entropy for a probability distribution indicates a greater uncertainty or more randomness in the distribution. Likewise, a smaller value indicates a more certain distribution i.e. less randomness`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use binary cross-entropy loss for classification models which output a probability p.\n",
    "\n",
    "> **Probability that the element belongs to class 1 (or positive class) = p**\n",
    "\n",
    "> **Then, the probability that the element belongs to class 0 (or negative class) = 1 - p**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as:\n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/log-loss.jpg.jpg)\n",
    "\n",
    "\n",
    "<center><b>This is also called <u>Log-Loss</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Log Loss = Log(\\frac{p}{1 - p}) = y$$\n",
    "\n",
    "\n",
    "<center><b> Here, y is a function of our input features. </b></center>\n",
    "\n",
    "\n",
    "* the probability p,can be find out using the sigmoid function. The sigmoid function is as follows:\n",
    "\n",
    "\n",
    "$$p = \\frac{e^y}{1 + e ^y}$$\n",
    "\n",
    "\n",
    "* Cross-entropy can be specified as the loss function in Keras by specifying ‘binary_crossentropy‘ when compiling the model.\n",
    "\n",
    "\n",
    "<center><b> model.compile( loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'] )</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Sigmoid-Cross-entropy loss\n",
    "\n",
    "The above cross-entropy loss requires that the predicted value is a probability. Generally, we calculate $scores = x*w + b$. Entering this value into the sigmoid function can compress the value range to (0,1).\n",
    "\n",
    "<img src=\".\\Images\\img9.png\">\n",
    "\n",
    "`It can be seen that the sigmoid function smoothes the predicted value`(such as directly inputting 0.1 and 0.01 and inputting 0.1, 0.01 sigmoid and then entering, the latter will obviously have a much smaller change value), which makes the predicted value of sigmoid-ce far from the label loss growth is not so steep.\n",
    "\n",
    "\n",
    "\n",
    "## 2.3. Softmax cross-entropy loss\n",
    "\n",
    "First, the softmax function can convert a set of fraction vectors into corresponding probability vectors. Here is the definition of softmax function\n",
    "\n",
    "<img src=\".\\Images_for_Activation&Loss\\img10.png\">\n",
    "\n",
    "As above, softmax also implements a vector of 'squashes' k-dimensional real value to the [0,1] range of k-dimensional, while ensuring that the cumulative sum is 1.\n",
    "\n",
    "According to the definition of cross entropy, probability is required as input.Sigmoid-cross-entropy-loss uses sigmoid to convert the score vector into a probability vector, and softmax-cross-entropy-loss uses a softmax function to convert the score vector into a probability vector.\n",
    "\n",
    "According to the definition of cross entropy loss.\n",
    "\n",
    "<img src=\".\\Images_for_Activation&Loss\\img11.png\">\n",
    "\n",
    "where $p(x)$ represents the probability that classification $x$ is a correct classification, and the value of $p$ can only be 0 or 1. This is the prior value\n",
    "\n",
    "$q(x)$ is the prediction probability that the $x$ category is a correct classification, and the value range is (0,1)\n",
    "\n",
    "So specific to a classification problem with a total of C types, then $p(x_j)$, $(0 <_{=} j <_{=} C)$ must be only 1 and C-1 is 0(because there can be only one correct classification, correct the probability of classification as correct classification is 1, and the probability of the remaining classification as correct classification is 0)\n",
    "\n",
    "Then the definition of softmax-cross-entropy-loss can be derived naturally.\n",
    "\n",
    "Here is the definition of softmax-cross-entropy-loss.\n",
    "\n",
    "<img src=\".\\Images_for_Activation&Loss\\img12.png\">\n",
    "\n",
    "Where $f_j$ is the score of all possible categories, and $f_{y_i}$ is the score of ground true class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hinge loss is `primarily used with Support Vector Machine (SVM)`.\n",
    "\n",
    "\n",
    "* It is intended for use with binary classification where the target values are in the set {-1, 1}.\n",
    "\n",
    "\n",
    "* The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
    "\n",
    "> NOTE- Make sure to change the label of class 0 to -1\n",
    "\n",
    "<center><b>change y from {0,1} to {-1,1} </b></center>\n",
    "<center><b>y[where(y == 0)] = -1 </b></center>\n",
    "\n",
    "\n",
    "* `Hinge loss function is given as:`\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/hinge.jpg.jpg)\n",
    " \n",
    " \n",
    "* `Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident.`\n",
    "\n",
    "\n",
    "* Hinge loss is specified as 'hinge' in the loss function during compile.\n",
    "\n",
    "<center><b> model.compile( loss='hinge' , optimizer=opt , metrics=['accuracy'] )</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Class Classification Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-Class classification are those predictive modeling problems where examples are assigned one of more than two classes.\n",
    "\n",
    "\n",
    "* The problem is often framed as predicting an integer value, where each class is assigned a unique integer value from 0 to (num_classes – 1). \n",
    "\n",
    "\n",
    "* The problem is often implemented as predicting the probability of the example belonging to each known class.\n",
    "\n",
    "\n",
    "* Emails can be classified into various other categories – Work, Home, Social, Promotions, etc. This is a Multi-Class Classification use case. In the Iris dataset based on Sepal length and Petal width, we can predict the class (Y) of the Iris flower – Setosa, Versicolor or Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multi-Class Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
    "\n",
    "\n",
    "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
    "\n",
    "\n",
    "* Cross-entropy can be specified as the loss function in Keras by specifying ‘categorical_crossentropy‘ when compiling the model.\n",
    "\n",
    "<center><b> model.compile( loss='categorical_crossentropy' , optimizer=opt , metrics=['accuracy'] ) </b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function requires that the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "\n",
    "<center><b> model.add( Dense( 3, activation='softmax' ) ) </b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the softmax activation function gives probability as the output, this means that the target variable must be one hot encoded. This is to ensure that each example has an expected probability of 1.0 for the actual class value and an expected probability of 0.0 for all other class values. This can be achieved using the `to_categorical()` Keras function.\n",
    "\n",
    "<center><b> one hot encode output variable </b></center>\n",
    "<center><b>y =  to_categorical( y ) </b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sparse Multiclass Cross-Entropy Loss\n",
    "\n",
    "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
    "\n",
    "\n",
    "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that `the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory`.\n",
    "\n",
    "\n",
    "* `Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training`.\n",
    "\n",
    "\n",
    "* Sparse cross-entropy can be used in keras for multi-class classification by using `sparse_categorical_crossentropy` when calling the compile() function.\n",
    "\n",
    "<center><b> model.compile( loss='sparse_categorical_crossentropy',  optimizer=opt, metrics=['accuracy'] ) </b></center>\n",
    "\n",
    "\n",
    "* The function requires that the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "\n",
    "<center><b> model.add( Dense(3, activation='softmax') ) </b></center>\n",
    "\n",
    "\n",
    "* `No one hot encoding of the target variable is required, a benefit of this loss function.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. KL-Divergence (Kullback Leibler Divergence Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a measure of how one probability distribution differs from a baseline distribution.\n",
    "\n",
    "\n",
    "* A KL divergence loss of 0 suggests the distributions are identical. \n",
    "\n",
    "\n",
    "* `In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.`\n",
    "\n",
    "\n",
    "* The KL divergence loss function is more commonly `used when using models that learn to approximate a more complex function `than simply multi-class classification, `such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input`. In this case, KL divergence loss would be preferred. \n",
    "\n",
    "\n",
    "* Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
    "\n",
    "\n",
    "* KL divergence loss can be used in Keras by specifying `kullback_leibler_divergence` in the compile() function.\n",
    "\n",
    "<center><b> model.compile( loss='kullback_leibler_divergence' , optimizer=opt,  metrics=['accuracy'] ) </b></center>\n",
    "\n",
    "\n",
    "* As with cross-entropy, the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.\n",
    "\n",
    "\n",
    "* Also, as with categorical cross-entropy, `we must one hot encode the target variable to have an expected probability of 1.0 for the class value and 0.0 for all other class values.`\n",
    "\n",
    "\n",
    "<center><b> one hot encode output variable </b></center>\n",
    "<center><b> y = to_categorical(y) </b></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
